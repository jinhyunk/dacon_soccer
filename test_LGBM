import os
import glob
import joblib
import numpy as np
import pandas as pd
from tqdm import tqdm
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
import warnings

warnings.filterwarnings('ignore')

# ==========================================
# 0. Configuration
# ==========================================
class Config:
    TRAIN_DIR = './data/train'
    VAL_DIR = './data/val'
    MODEL_DIR = './weights_lgbm'
    
    # ë°ì´í„° ìƒìˆ˜
    MAX_X = 105.0
    MAX_Y = 68.0
    MAX_TIME = 5700.0
    
    # LGBM íŒŒë¼ë¯¸í„° (ê³¼ì í•© ë°©ì§€ ìœ„ì£¼ ì„¤ì •)
    LGBM_PARAMS = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'n_estimators': 1000,
        'learning_rate': 0.05,
        'num_leaves': 31,         # íŠ¸ë¦¬ì˜ ë³µì¡ë„
        'max_depth': 7,           # íŠ¸ë¦¬ ê¹Šì´ ì œí•œ (ê³¼ì í•© ë°©ì§€)
        'min_child_samples': 20,  # ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ë°ì´í„° ìˆ˜
        'subsample': 0.8,         # ë°ì´í„° ìƒ˜í”Œë§ (Bagging)
        'colsample_bytree': 0.8,  # ì»¬ëŸ¼ ìƒ˜í”Œë§
        'reg_alpha': 0.1,         # L1 ê·œì œ
        'reg_lambda': 0.1,        # L2 ê·œì œ
        'random_state': 42,
        'n_jobs': -1
    }

# ==========================================
# 1. Feature Engineering (í•µì‹¬)
# ==========================================
def extract_features(df):
    """
    ì‹œê³„ì—´ ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ì„œ, ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ì„ ìœ„í•œ 
    1ê°œì˜ í–‰(Row)ìœ¼ë¡œ ìš”ì•½ëœ Featureë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ë°ì´í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ None ë°˜í™˜
    if len(df) < 1: return None
    
    # ì •ë ¬ (ì‹œê°„ ìˆœ)
    df = df.sort_values('time_seconds')
    
    # --- [1] íƒ€ê²Ÿ (ì •ë‹µ) ì¶”ì¶œ ---
    # ìš°ë¦¬ê°€ ë§ì¶°ì•¼ í•  ê²ƒì€ 'ë§ˆì§€ë§‰ ë™ì‘'ì˜ end_x, end_y ì…ë‹ˆë‹¤.
    target_x = df.iloc[-1]['end_x']
    target_y = df.iloc[-1]['end_y']
    
    # --- [2] ê¸°ë³¸ Features (ë§ˆì§€ë§‰ ìƒíƒœ) ---
    last_row = df.iloc[-1]
    
    features = {
        # ìœ„ì¹˜ ì •ë³´ (Normalize ì•ˆ í•´ë„ TreeëŠ” ì˜ ì°¾ì§€ë§Œ, ìŠ¤ì¼€ì¼ ë§ì¶¤)
        'start_x': last_row['start_x'],
        'start_y': last_row['start_y'],
        'time_seconds': last_row['time_seconds'],
        
        # ë²”ì£¼í˜• ì •ë³´ (Categoryë¡œ ë³€í™˜ ì˜ˆì •)
        'team_id': last_row['team_id'],
        'player_id': last_row['player_id'], # ì¹´ë””ë„ë¦¬í‹°ê°€ ë†’ì§€ë§Œ ì¼ë‹¨ í¬í•¨
        'type_name': last_row['type_name'], # Action Type
        
        # í†µê³„ ì •ë³´
        'phase_duration': df['time_seconds'].max() - df['time_seconds'].min(),
        'phase_event_count': len(df),
        'total_dist_x': df['end_x'].iloc[-1] - df['start_x'].iloc[0],
        'total_dist_y': df['end_y'].iloc[-1] - df['start_y'].iloc[0],
    }
    
    # --- [3] Lag Features (ê³¼ê±° ì´ë ¥) ---
    # ì§ì „(t-1) ë™ì‘ ì •ë³´
    if len(df) >= 2:
        prev_1 = df.iloc[-2]
        features.update({
            'prev1_start_x': prev_1['start_x'],
            'prev1_start_y': prev_1['start_y'],
            'prev1_end_x': prev_1['end_x'],
            'prev1_end_y': prev_1['end_y'],
            'prev1_type': prev_1['type_name'],
            'prev1_team': prev_1['team_id']
        })
    else:
        # ì—­ì‚¬ê°€ ì—†ìœ¼ë©´ -1 ë˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€
        features.update({
            'prev1_start_x': -1, 'prev1_start_y': -1,
            'prev1_end_x': -1, 'prev1_end_y': -1,
            'prev1_type': 'None', 'prev1_team': -1
        })
        
    # ì „ì „(t-2) ë™ì‘ ì •ë³´
    if len(df) >= 3:
        prev_2 = df.iloc[-3]
        features.update({
            'prev2_start_x': prev_2['start_x'],
            'prev2_start_y': prev_2['start_y'],
            'prev2_end_x': prev_2['end_x'],
            'prev2_end_y': prev_2['end_y'],
            'prev2_type': prev_2['type_name']
        })
    else:
        features.update({
            'prev2_start_x': -1, 'prev2_start_y': -1,
            'prev2_end_x': -1, 'prev2_end_y': -1,
            'prev2_type': 'None'
        })

    return features, target_x, target_y

def load_and_preprocess(data_dir, mode='train'):
    print(f"ğŸ”„ Loading CSVs from {data_dir}...")
    files = glob.glob(os.path.join(data_dir, '*.csv'))
    
    feature_list = []
    target_x_list = []
    target_y_list = []
    
    # ì§„í–‰ìƒí™© í‘œì‹œ
    for fpath in tqdm(files):
        try:
            df = pd.read_csv(fpath)
            # Feature Engineering
            feats, tx, ty = extract_features(df)
            
            if feats is not None:
                feature_list.append(feats)
                target_x_list.append(tx)
                target_y_list.append(ty)
        except Exception as e:
            continue
            
    # DataFrame ë³€í™˜
    X = pd.DataFrame(feature_list)
    y_x = np.array(target_x_list)
    y_y = np.array(target_y_list)
    
    # ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬
    cat_cols = ['type_name', 'prev1_type', 'prev2_type', 'team_id', 'prev1_team', 'player_id']
    for col in cat_cols:
        if col in X.columns:
            X[col] = X[col].astype('category')
            
    return X, y_x, y_y

# ==========================================
# 2. Training Engine
# ==========================================
def run_training():
    os.makedirs(Config.MODEL_DIR, exist_ok=True)
    
    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    print("ğŸ“‚ Preparing Train Data...")
    X_train, y_x_train, y_y_train = load_and_preprocess(Config.TRAIN_DIR)
    
    print("ğŸ“‚ Preparing Val Data...")
    X_val, y_x_val, y_y_val = load_and_preprocess(Config.VAL_DIR)
    
    print(f"âœ… Data Shape: Train {X_train.shape}, Val {X_val.shape}")
    
    # 2. ëª¨ë¸ í•™ìŠµ (X ì¢Œí‘œìš©, Y ì¢Œí‘œìš© ë”°ë¡œ í•™ìŠµ)
    print("\nğŸš€ Training Model for X Coordinate...")
    model_x = lgb.LGBMRegressor(**Config.LGBM_PARAMS)
    model_x.fit(
        X_train, y_x_train,
        eval_set=[(X_val, y_x_val)],
        eval_metric='rmse',
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]
    )
    
    print("\nğŸš€ Training Model for Y Coordinate...")
    model_y = lgb.LGBMRegressor(**Config.LGBM_PARAMS)
    model_y.fit(
        X_train, y_y_train,
        eval_set=[(X_val, y_y_val)],
        eval_metric='rmse',
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]
    )
    
    # 3. í‰ê°€ (Euclidean Distance)
    print("\nğŸ“Š Evaluating...")
    pred_x = model_x.predict(X_val)
    pred_y = model_y.predict(X_val)
    
    # ê±°ë¦¬ ì˜¤ì°¨ ê³„ì‚°
    diff_x = pred_x - y_x_val
    diff_y = pred_y - y_y_val
    dist = np.sqrt(diff_x**2 + diff_y**2)
    avg_dist = np.mean(dist)
    
    print(f"   >>> Validation Mean Distance Error: {avg_dist:.4f} m")
    
    # 4. ì €ì¥
    print("\nğŸ’¾ Saving Models...")
    joblib.dump(model_x, os.path.join(Config.MODEL_DIR, 'lgbm_model_x.pkl'))
    joblib.dump(model_y, os.path.join(Config.MODEL_DIR, 'lgbm_model_y.pkl'))
    
    # Feature Importance ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    print("\nğŸ” Top 5 Feature Importance (X-Model):")
    importances = pd.DataFrame({
        'feature': X_train.columns, 
        'importance': model_x.feature_importances_
    }).sort_values('importance', ascending=False)
    print(importances.head(5))

if __name__ == '__main__':
    run_training()